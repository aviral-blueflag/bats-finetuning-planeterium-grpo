{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"BatsResearch/planetarium\")\n",
    "#check structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training set for SFT + GRPO. 80 20\n",
    "test_data = dataset[\"test\"]\n",
    "#shuffle training set\n",
    "train_data = dataset[\"train\"].shuffle(seed=42)\n",
    "\n",
    "# Compute split sizes (80% GRPO, 20% SFT)\n",
    "total_train_size = len(train_data)\n",
    "sft_size = int(0.2 * total_train_size)   # 20% for SFT\n",
    "grpo_size = total_train_size - sft_size  # 80% for GRPO\n",
    "\n",
    "# Perform the split\n",
    "sft_data = train_data.select(range(sft_size))  # First 20% for SFT\n",
    "grpo_data = train_data.select(range(sft_size, total_train_size))  # Remaining 80% for GRPO\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"SFT: {len(sft_data)} samples\")\n",
    "print(f\"GRPO: {len(grpo_data)} samples\")\n",
    "print(f\"Test: {len(test_data)} samples (unchanged)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations for SFT finetuning on the 20% of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login to huggingface\n",
    "from huggingface_hub import login\n",
    "login(token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"google/gemma-2b-it\" \n",
    "#perhaps try a different model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA Configuration (Matching Paper)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,               # LoRA rank\n",
    "    lora_alpha=32,      # Scaling factor\n",
    "    lora_dropout=0.05,  # LoRA Dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_model\",\n",
    "    per_device_train_batch_size=1,  # Batch Size = 1\n",
    "    learning_rate=2e-5,  # Learning Rate = 2e-5\n",
    "    optim=\"adamw_torch\",  # Optimizer = AdamW_torch\n",
    "    betas=(0.9, 0.999),  # Betas = (0.9, 0.999)\n",
    "    eps=1e-8,  # Epsilon = 1e-8\n",
    "    weight_decay=0.01,  # Weight Decay = 0.01\n",
    "    max_length=1500,  # Max Sequence Length = 1500\n",
    "    num_train_epochs=3, # epochs, NEED HELP TO ADJUST THIS\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    bf16=True,  # faster?\n",
    "    gradient_accumulation_steps=1,  # Since batch size is 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=sft_data, \n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "sft_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for further use\n",
    "sft_trainer.save_model(\"./sft_model\")  # Save fine-tuned model\n",
    "tokenizer.save_pretrained(\"./sft_model\")  # Save tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# Load the SFT fine-tuned model for GRPO training\n",
    "sft_model_path = \"./sft_model\"\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    sft_model_path, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REWARD FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import planetarium.evaluate\n",
    "import planetarium\n",
    "\n",
    "def custom_reward_function(completions, ground_truths, domain_str=None, **kwargs):\n",
    "    rewards = []\n",
    "    \n",
    "    for generated_pddl, ground_truth_pddl in zip(completions, ground_truths):\n",
    "        parseable, solvable, equivalent = planetarium.evaluate(ground_truth_pddl, generated_pddl)\n",
    "        if equivalent:\n",
    "            reward = 1.0  # Correct PDDL -> highest reward\n",
    "        elif solvable:\n",
    "            reward = 0.5  # Solvable but incorrect -> somewhere in between reward\n",
    "        elif parseable:\n",
    "            reward = 0.2  # low\n",
    "        else:\n",
    "            reward = 0.0  # bad\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grpo trainer from huggingface\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the fine-tuned SFT model (previously saved at ./sft_model)\n",
    "sft_model_path = \"./sft_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(sft_model_path, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
    "\n",
    "# Define GRPO Training Configuration\n",
    "grpo_training_args = GRPOConfig(\n",
    "    output_dir=\"./grpo_model\", \n",
    "    learning_rate=1e-6,\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=16,  # Process multiple samples at once\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=512,  # Ensure max sequence length is within limits\n",
    "    num_generations=8,  # Generate multiple outputs per query\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape dataset\n",
    "print(grpo_data)\n",
    "#we need to make it into prompt/repsonse pairs\n",
    "from datasets import Dataset\n",
    "formatted_grpo_data = Dataset.from_dict({\n",
    "    \"prompts\": grpo_data[\"natural_language\"],  # Input NL description\n",
    "    \"ground_truth\": grpo_data[\"problem_pddl\"]  # Correct PDDL output\n",
    "})\n",
    "\n",
    "print(formatted_grpo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ze model\n",
    "\n",
    "# Initialize trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=\"path/to/sft_model\", #insert SFT PATH HERE\n",
    "    reward_funcs=custom_reward_function,\n",
    "    train_dataset=formatted_grpo_data,\n",
    "    args=grpo_training_args\n",
    ")\n",
    "\n",
    "# Train with GRPO\n",
    "grpo_trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "grpo_trainer.model.save_pretrained(\"./grpo_finetuned_model\")\n",
    "grpo_trainer.tokenizer.save_pretrained(\"./grpo_finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load fine-tuned GRPO model\n",
    "model_path = \"./grpo_finetuned_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Generate predictions\n",
    "def generate_pddl(prompts):\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=1500) # dont know this paramter\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Apply to test set\n",
    "generated_pddls = generate_pddl(test_data[\"prompts\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test set performance\n",
    "parseable_count = 0\n",
    "solvable_count = 0\n",
    "correct_count = 0\n",
    "\n",
    "# Loop through test samples\n",
    "for generated_pddl, ground_truth_pddl in zip(generated_pddls, test_data[\"ground_truth\"]):\n",
    "    parseable, solvable, equivalent = planetarium.evaluate(ground_truth_pddl, generated_pddl)\n",
    "    parseable_count += 1 if parseable else 0\n",
    "    solvable_count += 1 if solvable else 0\n",
    "    correct_count += 1 if equivalent else 0\n",
    "\n",
    "# Compute percentages\n",
    "total = len(test_data)\n",
    "parseable_pct = (parseable_count / total) * 100\n",
    "solvable_pct = (solvable_count / total) * 100\n",
    "correct_pct = (correct_count / total) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Parseable: {parseable_pct:.2f}%\")\n",
    "print(f\"Solvable: {solvable_pct:.2f}%\")\n",
    "print(f\"Correct PDDL: {correct_pct:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
